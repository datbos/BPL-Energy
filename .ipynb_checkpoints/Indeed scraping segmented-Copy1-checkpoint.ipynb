{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import urllib2 # Website connections\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline\n",
    "import nltk \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our first website parsing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    soup_obj = BeautifulSoup(site, \"lxml\") # Get the html from the site\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "        text = soup_obj.get_text() # Get the text from this\n",
    "        lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends \n",
    "    #of line\n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "    text = re.sub(\"[^a-zA-Z.+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "    # Also include + for C++\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "    # or not on the website)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning for the raw html is necessary to get the final terms we are looking for. Extract the relevant portions of the html, get the text, removes blank lines and line endings, removes unicode, and filters with regular expressions to include only words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def city_state(city = None, state = None):\n",
    "    ''' This function accepts the desired city and state for Indeed search and returns the\n",
    "    web site url. multi word cities are accoodated for like Salt Lake City'''\n",
    "    final_job = 'data+scientist'\n",
    "    # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    # Make sure the city specified works properly if it has more than one word(such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split()\n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,'%2C+', state]\n",
    "        # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "    \n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "    return final_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_jobs(final_site,city):\n",
    "    base_url = 'http://www.indeed.com'\n",
    "\n",
    "    try:\n",
    "        html = urllib2.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\") # Get the html from the first page\n",
    "    # Now find out how many jobs there were\n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8')    # Now extract the total number of jobs found\n",
    "                                                                            # The 'searchCount' object has this\n",
    "    job_numbers = re.findall('\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "    \n",
    "    if len(job_numbers) > 3:\n",
    "        #Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[2]) * 1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2])\n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "    \n",
    "    print 'There were', total_num_jobs, 'jobs found,', city_title  # Display how many jobs were found\n",
    "    num_pages = total_num_jobs / 10   # This will be how we know the number of times we need to iterate \n",
    "    print num_pages, \"pages\"                                  # over each new search result page\n",
    "    return num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_description(job_URLS):\n",
    "    \n",
    "    job_descriptions = []\n",
    "    pool = ThreadPool(4) \n",
    "        pool.map(urllib2.urlopen,job_URLS)\n",
    "#    for j in xrange(0, len(job_URLS)):\n",
    "#        print len(job_URLS)-j,\n",
    "#        final_description = text_cleaner(job_URLS[j])\n",
    "#        if final_description: #So that we only append when the website was accessed correctly\n",
    "#            job_descriptions.append(final_description)\n",
    "#        sleep(1)# So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "    return job_descriptions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_jobs(num_pages, final_site):\n",
    "    job_descriptions = []             # Store all our descriptions in this list\n",
    "    job_URLS = []\n",
    "    base_url = 'http://www.indeed.com'\n",
    "\n",
    "    for i in xrange(1, num_pages + 1): #Loop through all of our search result pages\n",
    "        print 'Getting page', i\n",
    "        start_num = str(i * 10)       # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        print current_page\n",
    "\n",
    "        \n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "        html_page = urllib2.urlopen(current_page).read() # Get the page        \n",
    "        page_obj = BeautifulSoup(html_page, \"lxml\")      # Locate all of the job links\n",
    "\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "        for link in job_link_area.find_all('a'):\n",
    "            if link.get('href') != None:                \n",
    "                job_URLS.append(base_url + link.get('href'))\n",
    "                \n",
    "                #job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')]\n",
    "                # Get the URLS for the jobs\n",
    "        job_URLS = filter(lambda x: 'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "\n",
    "\n",
    "\n",
    "    return job_URLS    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city / state and look\n",
    "    for all new job postings on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage\n",
    "    for each skill is then displayed at the end of the collation.\n",
    "    Inputs: The location 's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search(this can take a while !!!).\n",
    "    Input the city / state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for\n",
    "    a data scientist.\n",
    "    '''\n",
    "    #final_site = city_state(city, state)\n",
    "    sample = city_state(city,state)\n",
    "    \n",
    "    sample_no = number_jobs(sample,city)\n",
    "            \n",
    "    sample_jobs = find_jobs(sample_no,sample) \n",
    "               \n",
    "    job_descript = get_description(sample_jobs)\n",
    "    \n",
    "    print 'Done with collecting the job postings!'\n",
    "    print 'There were', len(job_descript), 'jobs successfully found.'\n",
    "    \n",
    "    city_title = city\n",
    "    doc_frequency = Counter()  # This will create a full counter of our terms.\n",
    "    [doc_frequency.update(item) for item in job_descript]  # List comp\n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "\n",
    "    # Obtain our key terms and store them in a dict. \n",
    "    # These are the key data science skills we are looking for\n",
    "    prog_lang_dict = Counter({\n",
    "        'R': doc_frequency['r'],\n",
    "        'Python': doc_frequency['python'],\n",
    "        'Java': doc_frequency['java'],\n",
    "        'C++': doc_frequency['c++'],\n",
    "        'Ruby': doc_frequency['ruby'],\n",
    "        'Perl': doc_frequency['perl'],\n",
    "        'Matlab': doc_frequency['matlab'],\n",
    "        'JavaScript': doc_frequency['javascript'],\n",
    "        'Scala': doc_frequency['scala']\n",
    "    })\n",
    "    analysis_tool_dict = Counter({\n",
    "        'Excel': doc_frequency['excel'],\n",
    "        'Tableau': doc_frequency['tableau'],\n",
    "        'D3.js': doc_frequency['d3.js'],\n",
    "        'SAS': doc_frequency['sas'],\n",
    "        'SPSS': doc_frequency['spss'],\n",
    "        'D3': doc_frequency['d3']\n",
    "    })\n",
    "    hadoop_dict = Counter({\n",
    "        'Hadoop': doc_frequency['hadoop'],\n",
    "        'MapReduce': doc_frequency['mapreduce'],\n",
    "        'Spark': doc_frequency['spark'],\n",
    "        'Pig': doc_frequency['pig'],\n",
    "        'Hive': doc_frequency['hive'],\n",
    "        'Shark': doc_frequency['shark'],\n",
    "        'Oozie': doc_frequency['oozie'],\n",
    "        'ZooKeeper': doc_frequency['zookeeper'],\n",
    "        'Flume': doc_frequency['flume'],\n",
    "        'Mahout': doc_frequency['mahout']\n",
    "    })\n",
    "    database_dict = Counter({\n",
    "        'SQL': doc_frequency['sql'],\n",
    "        'NoSQL': doc_frequency['nosql'],\n",
    "        'HBase': doc_frequency['hbase'],\n",
    "        'Cassandra': doc_frequency['cassandra'],\n",
    "        'MongoDB': doc_frequency['mongodb']\n",
    "    })\n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict\n",
    "    # Combine our Counter objects\n",
    "    print overall_total_skills\n",
    "    final_frame = pd.DataFrame(overall_total_skills.items(), columns = ['Term', 'NumPostings'])\n",
    "    # Convert these terms to a dataframe\n",
    "    # Change the values to reflect a percentage of the postings\n",
    "    final_frame.NumPostings = (final_frame.NumPostings) * 100 / len(job_descript)\n",
    "    # Gives percentage of job postings# having that term\n",
    "    # Sort the data for plotting purposes\n",
    "    #final_frame.sort(key=takeSecond, reverse=True)  # Get it ready for a bar plot\n",
    "    final_frame.sort_values('NumPostings', ascending = False, inplace = True)\n",
    "    \n",
    "    final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, title = 'Percentage of Data Scientist Job Ads with a Key Skill, ' + city_title)\n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure()     # Have to convert the pandas plot object to a matplotlib object\n",
    "    return fig, final_frame  # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 385 jobs found, new york\n",
      "38 pages\n",
      "Getting page 1\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=10\n",
      "Getting page 2\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=20\n",
      "Getting page 3\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=30\n",
      "Getting page 4\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=40\n",
      "Getting page 5\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=50\n",
      "Getting page 6\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=60\n",
      "Getting page 7\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=70\n",
      "Getting page 8\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=80\n",
      "Getting page 9\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=90\n",
      "Getting page 10\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=100\n",
      "Getting page 11\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=110\n",
      "Getting page 12\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=120\n",
      "Getting page 13\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=130\n",
      "Getting page 14\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=140\n",
      "Getting page 15\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=150\n",
      "Getting page 16\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=160\n",
      "Getting page 17\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=170\n",
      "Getting page 18\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=180\n",
      "Getting page 19\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=190\n",
      "Getting page 20\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=200\n",
      "Getting page 21\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=210\n",
      "Getting page 22\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=220\n",
      "Getting page 23\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=230\n",
      "Getting page 24\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=240\n",
      "Getting page 25\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=250\n",
      "Getting page 26\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=260\n",
      "Getting page 27\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=270\n",
      "Getting page 28\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=280\n",
      "Getting page 29\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=290\n",
      "Getting page 30\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=300\n",
      "Getting page 31\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=310\n",
      "Getting page 32\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=320\n",
      "Getting page 33\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=330\n",
      "Getting page 34\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=340\n",
      "Getting page 35\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=350\n",
      "Getting page 36\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=360\n",
      "Getting page 37\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=370\n",
      "Getting page 38\n",
      "http://www.indeed.com/jobs?q=%22data+scientist%22&l=new+york%2C+ny&start=380\n",
      "354 353 352 351 350 349 348 347 346 345 344 343 342 341 340 339 338 337 336 335 334 333 332 331 330 329 328 327 326 325 324 323 322 321 320 319 318 317 316 315 314 313 312 311 310 309 308 307 306 305 304 303 302 301 300 299 298 297 296 295 294 293 292 291 290 289 288 287 286 285 284 283 282 281 280 279 278 277 276 275 274 273 272 271 270 269 268 267 266 265 264 263 262 261 260 259 258 257 256 255 254 253 252 251 250 249 248 247 246 245 244 243 242 241 240 239 238 237 236 235 234 233 232 231 230 229 228 227 226 225 224 223 222 221 220 219 218 217 216 215 214 213 212 211 210 209 208 207 206 205 204 203 202 201 200 199 198 197 196 195 194 193 192 191 190 189 188 187 186 185 184 183 182 181 180 179 178 177 176 175 174 173 172 171 170 169 168 167 166 165 164 163 162 161 160 159 158 157 156 155 154 153 152 151 150 149 148 147 146 145 144 143 142 141 140 139 138 137 136 135 134 133 132 131 130 129 128 127 126 125 124 123 122 121 120 119 118 117 116 115 114 113 112 111 110 109 108 107 106 105 104 103 102 101 100 99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 81 80 79 78 77 76 75 74 73 72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'chunks' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c9f5a4d3b07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseattle_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'new york'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ny'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mseattle_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-fae3ce9e760d>\u001b[0m in \u001b[0;36mskills_info\u001b[0;34m(city, state)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msample_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mjob_descript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Done with collecting the job postings!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-7cc4aaa20440>\u001b[0m in \u001b[0;36mget_description\u001b[0;34m(job_URLS)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_URLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_URLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mfinal_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_URLS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinal_description\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#So that we only append when the website was accessed correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mjob_descriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-6acd464d9db0>\u001b[0m in \u001b[0;36mtext_cleaner\u001b[0;34m(website)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mchunk_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;31m# Need to fix spacing issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchunk_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get rid of all blank lines and ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#of line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Now clean out all of the unicode junk (this line works great!!!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'chunks' referenced before assignment"
     ]
    }
   ],
   "source": [
    "seattle_info = skills_info(city = 'new york', state = 'ny') \n",
    "seattle_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
